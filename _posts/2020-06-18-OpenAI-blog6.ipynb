{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Scholars: Sixth Steps - A PyTorch Tutorial\n",
    "\n",
    "Over the course of the [OpenAI Scholars program](https://openai.com/blog/openai-scholars-spring-2020/), my mentor [Johannes](https://jotterbach.github.io/) has given me a number of very helpful pair programming tutorials to improve my facility with `PyTorch`.\n",
    "\n",
    "Since they were useful to me, I see no reason not to share them with you. :-)\n",
    "\n",
    "This is the very first tutorial, which we did a couple of months back. It's a common beginners' machine learning example: Training a logistic regression classifier to separate digits of the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "If you're interested in a more verbose discussion about datasets and dataloaders in `PyTorch`, I recommend an [earlier blog post I wrote on this topic](https://katarinaslama.github.io/2020/04/02/OpenAI-blog3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the requisite packages\n",
    "\n",
    "Importing the packages you'll need is usually a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as tr\n",
    "import torchvision.utils as tvu\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([`tqdm`](https://pypi.org/project/tqdm/) is a really cool package that I learned about during the program. It makes a handy progress bar for your network training. Also, the name of the package derives from the Arabic word root [\"qadama\", to precede](https://cooljugator.com/ar/%D9%82%D8%AF%D9%85), and here it means \"progress\". So \"taqaddum\" helps you track your network's progress.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torvision datasets\n",
    "\n",
    "The `torchvision.datasets` package (here, `ds`) has a method for downloading a nicely curated version of the MNIST dataset, which we can use for learning. We start by defining a root directory, where we will store the data. (It does not need to exist on your computer yet. `PyTorch` will create it for you if you don't have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '~/pp_mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to apply some transforms to our data, including converting it to `PyTorch` tensors, and rescaling it to range from -1 to 1. We define the transforms here, but do not apply them yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tr.Compose([\n",
    "    tr.ToTensor(),\n",
    "    tr.Lambda(lambda x: (2. * x - 1).reshape(-1, ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the MNIST data the `torchvision.datasets` package. We do so separately for the training and test sets. We apply the transforms that we defined above. We set the `download` flag to `True` because we have not downloaded this data before. (If you already have the data in your root directory, you can save some time and compute by setting it to `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds.MNIST(root=root_directory,\n",
    "                    train=True,\n",
    "                    transform=transforms,\n",
    "                    download=True)\n",
    "\n",
    "test_ds = ds.MNIST(root=root_directory,\n",
    "                   train=False,\n",
    "                   transform=transforms,\n",
    "                   download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torvision dataloaders\n",
    "\n",
    "Now, we define dataloaders for our training and test sets, so that we can iterate through the MNIST examples in batches and in randomized order. Sometimes, the last batch is incomplete (if the training set size doesn't happen to be a multiple of your batch size), and this can cause problems if you don't consider that in subsequent code. An easy way to mitigate against this problem is by setting the `drop_last` flag to `True`. As you might have guessed, that excludes the last batch from your training and evaluation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tud.DataLoader(train_ds,\n",
    "                              batch_size=128,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)\n",
    "\n",
    "test_loader = tud.DataLoader(test_ds,\n",
    "                             batch_size=128,\n",
    "                             shuffle=False,\n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some examples from your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, it's a good idea to take a look at (at least some of) it.\n",
    "The first step is to extract some examples from the trainloader, and to do so, we turn it into an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call the `next` method on the train iterator, to get a batch of training examples. The zeroeth element has the data (the actual MNIST digits), and the first element has the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_batch, these_labels = train_iterator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` has a handy method called `make_grid`, which allows us to visualize examples on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvu_fig = tvu.make_grid(this_batch[:16, ...].reshape(16, 1, 28, 28),\n",
    "                        nrow=4,\n",
    "                        normalize=True,\n",
    "                        scale_each=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually plot the figure, we need to use the `matplotlib` library (`plt` below), and that library only really plays nice with numpy, so we convert our `PyTorch` tensor to `NumPy` in order to plot it. Images are formatted differently in `PyTorch` than `NumPy`, so we also change the order of the image dimensions, using the `permute` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tvu_fig.permute(1, 2, 0).data.numpy()) \n",
    "plt.axis('off')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the labels, corresponding to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 5, 1, 9, 0, 1, 5, 9, 5, 8, 0, 3, 8, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(these_labels[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See! We have the data. And the labels make sense. :-) Don't you wish all data collection was this easy?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, and we have confirmed to ourselves that it looks as we would hope, we can define a model, which we will use to learn a mapping between the image data and the corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model to start with when you have discrete labels (in the case of the MNIST dataset, we have 10, one for each digit), is a logistic regression. But notice that the way we define a model in `PyTorch` (as below) is very general, and you can plug in all sorts of exciting models into this general code skeleton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defining features of a `PyTorch` module are the `__init__` and `forward` methods. In the `init` method, we define the model, i.e. the network architecture, by specifying any functions that we are planning to use.\n",
    "In the `forward` method, we specify *how*, i.e. in what order, we want to apply those functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.num_classes = 10\n",
    "        self.logits = nn.Sequential(\n",
    "            nn.Linear(28**2, # the shape of each image is 28*28 pixels\n",
    "                      self.num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.logits(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the evaluation loop (test process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the evaluation loop. The evaluation loop will get called at the end of each training epoch (see the section, **Train the model**, below). Its purpose is to evaluate how well our model is doing on the test set. What happens in the evaluation loop does not affect the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, test_loader):\n",
    "    # Set the model to evaluation mode. This saves a lot of computational work, because we don't need to compute\n",
    "    # when we evaluate the model.\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize a list to store accuracy values\n",
    "    acc = []\n",
    "    # Iterate through the test loader. Use tqdm to display our progress.\n",
    "    for this_batch, these_labels in tqdm(test_loader, \n",
    "                                         leave=False):\n",
    "        # Get model predictions for each batch.\n",
    "        pred = model(this_batch)\n",
    "        # Get labels out of the predictions, by simply extracting the maximal value from the real-valued predictions.\n",
    "        pred_lbl = pred.argmax(dim=-1)\n",
    "        # For each training example, store whether we predicted the correct label in our accuracy (acc) list\n",
    "        acc += (pred_lbl == these_labels).numpy().tolist()\n",
    "    # When we've gone through all the batches, return the mean of the accuracy counts.\n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to train our model! To do so, we first create a model instance by calling our `LogReg` module from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogReg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create an optimizer, whom we will name `opt`. We choose the `Adam` optimizer, but `PyTorch` [has many optimizers that you can choose from](https://pytorch.org/docs/stable/optim.html). We select a [learning rate](https://en.wikipedia.org/wiki/Learning_rate), `lr`, of 0.001. Notice that the learning rate is itself a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) that you can optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), \n",
    "                       lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a criterion, `crit`, also known as a [loss function](https://en.wikipedia.org/wiki/Loss_function), which will help us determine how well the model is doing and in what direction to update its weights. We choose [`CrossEntropyLoss`](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html), which is usually a good choice for categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train for 10 [epochs](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/). In one epoch you loop through all of your training data once. You might try training with more epochs and see if that helps performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we loop through our epochs to train the model, while monitoring performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446635087b3449baa7ffd2d7ad322db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7297ced13864238bcd6b2d34c7f2814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in tqdm(range(epochs), \n",
    "              leave=True):\n",
    "    # We set the model to \"train\" mode, which means that we will compute the gradient that we need, and update the weights.\n",
    "    model.train()\n",
    "    # We loop through all the batches in our train loader.\n",
    "    for this_batch, these_labels in tqdm(train_loader, leave=False):\n",
    "        # We want to set the gradient to 0 between each batch (weight update). Otherwise, PyTorch accumulates them, \n",
    "        # and bad things happen.    \n",
    "        opt.zero_grad()\n",
    "        # We compute model predictions by applying the model to a batch of data.\n",
    "        pred = model(this_batch)\n",
    "        # Once, we have the predictions, we calculate how bad they are by computing the loss (CrossEntropyLoss from above)\n",
    "        # between the predictions and the true labels.\n",
    "        loss = crit(pred, these_labels)\n",
    "        # We use the \"backward\" command to compute the gradient that we need for updating the model.\n",
    "        loss.backward()\n",
    "        # We use the \"step\" command to actually update the model. \n",
    "        opt.step()\n",
    "    \n",
    "    # Now that we have an updated model, after having seen all the data once, we evaluate how well the model is doing\n",
    "    # on the heldout test set, by calling our \"eval_loop\" from above.\n",
    "    acc = eval_loop(model, test_loader)\n",
    "    \n",
    "    # And we print the results. If things are going well, the loss should be going down, and the accuracy should be going up.\n",
    "    print('**epoch**: ', e)\n",
    "    print('training loss: ', loss.detach().numpy())\n",
    "    print('test accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and that's it for this first demo. I suggest that you try changing something in this notebook to see what happens. Can you make the accuracy go up more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
